\section{AI and Machine Learning Contributions}\label{ml-sec}

As discussed in \Cref{automated-sec}, the ETP made extensive use of automated theorem provers in completing the primary goal of determining and then formalizing all the implications between the specified equational laws.  In contrast, we were only able to utilize modern large language models (LLMs) in a fairly limited fashion.  Such models were useful in writing initial code for graphical user interfaces that we discuss further in \Cref{sec:gui-sec}, as well as performing some code autocompletion (using tools such as \emph{Github Copilot}) when formalizing an informal proof in \emph{Lean}.  In one instance, \emph{ChatGPT} was used\footnote{\url{https://chatgpt.com/share/670ce7db-8a44-800d-a5dc-8462c12eca3b}} to guess a complete rewriting system for the law $\x \op ((y \op y) \op \z) \formaleq x \op y$ \eqref{eq1659} which could then be formally verified, thus resolving all implications from this equation. However, in most of the difficult implications that resisted automated approaches, we found that LLMs did not provide useful suggestions beyond what the human participants could already propose.

On the other hand, we found that machine learning (ML) methods showed some promise of being able to heuristically predict the truth value of portions of the implication graph, as we shall now discuss\footnote{For some discussion of other ML experiments performed during the project, see \url{https://leanprover.zulipchat.com/\#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results}.}.

\subsection{Convolutional neural network model for the implication graph}

To model the implication graph, we used a convolutional neural network (CNN). For each pair of equations $(p,q)$, the input of the CNN consisted on a character-level tokenization (no vectorization) of the two equations, and the output of the CNN was a yes/no label depending on whether $p$ implies $q$. The CNN processed the input data using a 5-layer architecture, each layer composed of a 1-D convolution, followed by batch normalization and rectified linear unit activation functions (\cite{Goodfellow-et-al-2016}). After the last convolutional layer, a flattening layer and a softmax activation function were used to obtain the output of the network, i.e., the prediction of the implication for the input pair of equations. Note that we trained our models with the 16/10/2024 version of the (infinite) implication graph, for which 362 of the hardest implications (less than 0,002\% of the total) were still unknown.

\smallskip

Prior to training the CNN, we divided the data into training (60\%),validation (20\%), and test (20\%) subsets. The CNN was implemented in TensorFlow 2.9.0 (\cite{tensorflow2015-whitepaper}) and trained on an NVIDIA 3080 Ti GPU with the following configuration: the binary cross-entropy as the loss function to minimize, the Adam method with an initial learning rate of $10^{-3}$ for the adjustment of network weights, a batch size of 1024 with random shuffling, a learning rate reduction by a factor of 2 after 15 epochs without improvement in the validation loss, and early stopping if no improvement occurred for 40 epochs, being the model with the lowest validation loss retained as the final CNN.
The final CNN was evaluated on the test set, reaching prediction accuracy of 99.7\%, which means that the model mis-classified around 66k of the 22 million implications. Since this accuracy was somewhat surprising for such a small and “simple” model, as a control we generated a random label (yes/no) for each pair of equations, then trained the CNN on this data with 60\%/20\%/20\% training/validation/test percentages, resulting in a 49.99\% accuracy, as expected from a nonbiased model.

\smallskip

It could be the case that the high accuracy of our CNN model was mostly due to it learning the transitivity of the implication relation, as opposed to it discovering patterns in the identities. To clarify this point, it was proposed to train our CNN model either on a random poset or on the equational poset with vertices and labels permuted, and check whether a similar accuracy was achieved. This experiment has not been performed yet.

\smallskip

In any case, since there are around 600k explicit implications from which the rest can be derived by transitivity (2.7\% of the total), if the CNN was learning transitivity it should perform well with a really small training dataset. Accordingly, we trained and assessed a CNN model with a 5\%/5\%/90\% training/validation/test proportion, with a resulting 99.6\% accuracy on test (the process finishing in under 20 minutes). But the high accuracy is maintained with even smaller training datasets, as evidenced in the following table:
\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \textbf{Training/validation/test proportion (\%)} & \textbf{Prediction accuracy (\%)} \\\hline\hline
  60/20/20 & 99.7 \\\hline
  5/5/90 & 99.6 \\\hline
  1/1/98 & 99.3 \\\hline
  0.5/0.5/99 & 98.9 \\\hline
  0.1/0.1/99.8 & 92.2 \\
  \hline
\end{tabular}
\caption{Prediction accuracy as function of size of training set}
\end{table}

\noindent Since these training datasets were sensibly smaller than the subset of explicit implications, and were not carefully chosen from the poset extremes but taken randomly, we can conclude that even if the CNN was learning transitivity, probably this alone is not enough to explain the high accuracy achieved by the CNN model.

\smallskip

Since sometimes machine learning is announced as a form of data compression, let us now comment on the level of data compression achieved by our CNN model. In the table below we compare the sizes of three different encodings of the implication graph: a) As the simplest approach, we can encode the full implication graph in one file as labelled pairs of equations of the form ($p$, $q$, yes/no). b) On the other extreme, we can encode it as a bit table not containing neither the explicit equation expressions nor their numbers, but just a 1/0 label for each point with coordinates ($p$,$q$), together with a table mapping each number to its corresponding equation expression and a small script to recover the file in a). c) Finally, we can encode it in the complete files of the CNN model produced by TensorFlow 2.9.0. In addition, we can either consider these files in their raw form, or we can highly (but losslessly) compress them to achieve a rough comparison of their actual information content; accordingly, in the table below we also include the sizes of the encoding models when compressed with 7-zip LZMA2 ultra compression with a 1536MB dictionary size and a 273 word size.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \textbf{Encoding model} & \textbf{Uncompressed size} & \textbf{Compressed size} \\\hline\hline
  Labelled pairs of equations & 1.5GB & 9MB \\\hline
  Bit table & 42MB & 40KB \\\hline
  CNN (99.7\% accuracy) & 1.34MB & 700KB \\
  \hline
\end{tabular}
\caption{Size of different encodings for the implication graph}
\end{table}

\smallskip

\noindent As we see, the information in the CNN model is more than 13 times less than in the labelled pairs model, while it is more than 18 times that of the bit table model. We also note that the CNN model in its raw form is already quite incompressible, and more than 30 times smaller than the raw bit table.

\smallskip

Lastly, take also into account that our CNN model does not only encode the implication graph up to order 4 (with a 0.03\% of noise) but a priori may also be able to predict it for higher orders with a significant accuracy, and could thus be used to guide and speed up the determination of the implication graph up to order 5, by letting ATPs focus first on the CNN’s predicted status of the studied implication.


\subsection{Graph ML: Directed link prediction on the implication graph}
\label{sec:dlp}

We experimented with various Graph Neural Network (GNN) autoencoders to predict missing edges,
providing a way to estimate the truth value of unproven implications. To assess these models,
we defined three test sets focusing on edge existence, directionality, and bidirectionality.
The results give insight into how these models handle dense, directed graphs like ours.
A more detailed report follows below.


\subsubsection{Motivation}

Directed Link Prediction~\cite{Kipf2016} is a method enabling machine learning and deep learning
models to predict missing edges in a directed graph. For our implication graph, this translates to
predicting the truth values of unproven implications. This task serves as a necessary first step
for advancing in the following directions:

\begin{enumerate}
    \item \textbf{Reasoning over mathematical knowledge graphs:} Recent advancements allow language
    models to integrate information from multiple modalities. For example,
    \cite{Zhang2022, Yasunaga2022} share information between corresponding layers of Language Models (LMs)
    and Graph Neural Networks (GNNs), enabling simultaneous learning from text corpora and graph-structured
    data within the same expert domain. By leveraging both modalities, the language model can better
    \emph{structure} its knowledge and respond to complex queries. This dual learning process combines
    masked language modeling for text with link prediction on the graph, highlighting the importance
    of link prediction for robust reasoning.
    \item \textbf{Higher-order implication graphs:} Our implication graph currently represents only
    implications of the form $p \implies q$, not more complex ones like $(p \land r) \implies q$.
    Extending to such higher-order edges would likely involve connecting sets of nodes, thereby
    requiring hypergraph representations. For a systematic overview, see \cite{Kivela2014}.
    While specific hypergraph neural architectures exist~\cite{Feng2019}, we believe it is still
    conceptually important to apply Directed Link Prediction to simpler implication graphs first,
    providing insights and guiding principles that can anticipate challenges in higher-order graph
    representation learning.
\end{enumerate}

\subsubsection{Data}

We used \href{https://github.com/teorth/equational_theories/blob/main/data/2024-10-20-edge_list.csv.zip}{this edge list},
generated on October 20, 2024, with the following commands:

\begin{verbatim}
    lake exe extract_implications outcomes > data/tmp/outcomes.json
    scripts/generate_edgelist_csv.py
\end{verbatim}

The structure of the implication graph is summarized below:

\begin{verbatim}
    graph_summary = {
        "total_nodes": 4694,
        "total_directed_edges": 8178279,
        "edge_density_percentage": 37,  # % of possible edges that exist
        "bidirectional_edges": 2475610,
        "bidirectional_percentage": 30  # % of all edges that are bidirectional
    }
\end{verbatim}

Below is a summary of the edge types in the graph:

\begin{verbatim}
    edge_counts = {
        "explicit_conjecture_false": 92,
        "explicit_proof_false": 582316,
        "explicit_proof_true": 10657,
        "implicit_conjecture_false": 142,
        "implicit_proof_false": 13272681,
        "implicit_proof_true": 8167622,
        "unknown": 126
    }
\end{verbatim}

Edges are labeled according to the following scheme:

\begin{verbatim}
    edge_labels = {
        "implicit_proof_true": 1,
        "explicit_proof_false": 0,
        "implicit_proof_false": 0,
        "explicit_proof_true": 1,
        "explicit_conjecture_false": 0,
        "implicit_conjecture_false": 0,
        "unknown": 0
    }
\end{verbatim}

The \texttt{unknown} class contains a very small number of edges (126), so their impact on the
training phase is expected to be negligible. Future approaches might address this class by
excluding \texttt{unknown} edges from the training set.

\subsubsection{Methods}

Consider a directed graph $G = (V, E)$ where $E = \{(u,v) \mid u, v \in V\}$ is the edge set and
$|V| = n$. We assume that each node is associated with a feature vector, resulting in an
$X \in \mathbb{R}^{n \times f}$ feature matrix.

We define the existing edges $(a, b) \in E$ as \emph{positives} and the non-existing edges
$(c, d) \notin E$ as \emph{negatives}.

Intuitively, performing Directed Link Prediction (DLP) on $G$ involves randomly splitting $E$
into three disjoint sets: $E_{\text{train}}$, $E_{\text{val}}$, and $E_{\text{test}}$, such that:

\begin{itemize}
    \item $E_{\text{train}}$ is the training set,
    \item $E_{\text{val}}$ is the validation set,
    \item $E_{\text{test}}$ is the test set,
    \item and $E = E_{\text{train}} \dot{\cup} E_{\text{val}} \dot{\cup} E_{\text{test}}$.
\end{itemize}

The model then learns from $G_{\text{train}} = (V, E_{\text{train}})$ to map the topological and
feature-related information of two nodes $u$ and $v$ to a probability $p_{uv}$ that
$(u, v) \in E_{\text{test}}$.

However, this setup presents two key issues among others:

\begin{enumerate}
    \item The model learns only from \emph{positives}, so it cannot recognize \emph{negatives}.
    \item The model is evaluated only on \emph{positives}, preventing us from measuring its ability
    to identify \emph{negatives}.
\end{enumerate}

To address these limitations, we adopted the setup proposed in~\cite{Salha2019}.
Specifically, we redefined $E_{\text{train}}$ to $E_{\text{train}}^p$ (\emph{positives}) and introduced:

\[
E_{\text{train}} = E_{\text{train}}^p \dot{\cup} E_{\text{train}}^n
\]

where $E_{\text{train}}^n$ includes all possible \emph{negatives} in $G_{\text{train}} = (V, E_{\text{train}}^p)$.
The model is now required to predict the non-existence of edges in $E_{\text{train}}^n$.

Similarly, if we redefine $E_{\text{test}}$ as follows:

\[
E_{\text{test}} = E_{\text{test}}^p \dot{\cup} E_{\text{test}}^n
\]

where $E_{\text{test}}^n$ is a \emph{random} sample of \emph{negatives}, the model's evaluation
would fail to capture two crucial aspects:

\begin{enumerate}
    \item The model's ability to distinguish $(u,v)$ from $(v,u)$ for all $(u,v) \in E_{\text{test}}^p$.
    \item The model's ability to identify bi-implications.
\end{enumerate}

These limitations arise from the random selection of negative edges in $E_{\text{test}}^n$.
To address this, we define three distinct test sets: $E_{\text{test}}^G$, $E_{\text{test}}^D$,
and $E_{\text{test}}^B$, to evaluate different facets of the model’s performance:

\begin{itemize}
    \item \textbf{General Test Set} ($E_{\text{test}}^G$):
    Here, $E_{\text{test}} = E_{\text{test}}^p \dot{\cup} E_{\text{test}}^n$, where $E_{\text{test}}^n$
    is a random sample of non-existent edges with the same cardinality as $E_{\text{test}}^p$.
    This set assesses the model's ability to detect the presence of edges, regardless of direction.
    A model that cannot distinguish edge direction may still perform well on this set, highlighting
    the need for the following two additional test sets.
    \item \textbf{Directional Test Set} ($E_{\text{test}}^D$):
    Defined as $E_{\text{test}}^{\text{up}} \dot{\cup} \tilde{E}_{\text{test}}^{\text{up}}$,
    where $E_{\text{test}}^{\text{up}}$ consists of unidirectional edges in $E_{\text{test}}^p$,
    and $\tilde{E}_{\text{test}}^{\text{up}}$ contains their reverses (negatives by construction).
    This set evaluates the model's ability to recognize edge direction, making it suitable for
    assessing direction-sensitive models.
    \item \textbf{Bidirectional Test Set} ($E_{\text{test}}^B$):
    Defined as $E_{\text{test}}^{\text{bp}} \dot{\cup} E_{\text{B}}^{\text{n}}$,
    where $E_{\text{test}}^{\text{bp}}$ contains one direction of all bidirectional edges in $E_{\text{test}}^p$,
    and $E_{\text{B}}^{\text{n}} \subset \tilde{E}$ includes a subset of their reverses.
    This set evaluates the model's ability to identify bi-implications, as each edge in $E_{\text{test}}^B$
    has a reverse that is positive, but only half are bidirectional in practice.
\end{itemize}

We tested the following models:

\begin{itemize}
    \item \textbf{GAE}~\cite{Kipf2016}
    \item \textbf{Gravity-GAE}~\cite{Salha2019}
    \item \textbf{Source/Target-GAE}~\cite{Salha2019}
    \item \textbf{DiGAE}~\cite{Kollias2022}
    \item \textbf{MagNet}~\cite{Zhang2021}
\end{itemize}

All these models are graph-based autoencoders with distinct encoder-decoder architectures.
Notably, GAE is the only model structurally unable to differentiate edge directions.
Each model outputs the probability that an ordered pair of nodes has a directed edge between them,
with nodes represented using one-hot encodings as features.

We trained the models using Binary Cross Entropy as the loss function, balancing the contribution
of positive and negative edges through re-weighting. On the \emph{General} test set, we evaluated
the following metrics:

\begin{itemize}
    \item \textbf{AUC} (Area Under the ROC Curve): Measures the probability that the model ranks a
    random positive edge higher than a random negative edge. Higher values indicate better
    discrimination between positive and negative edges.
    \item \textbf{AUPRC} (Area Under Precision-Recall Curve): Assesses model performance,
    particularly in cases of class imbalance. AUPRC is more robust to imbalanced data than AUC.
    \item \textbf{Hits@K}: Evaluates the fraction of times a positive edge is ranked within the
    top $K$ positions among personalized negative samples~\cite{Li2023}. Briefly, given a positive
    edge, its $M$ personalized negative samples are $M$ negative edges with the same head but
    different tails. We calculate Hits@K for $K = 1, 3, 10$ to assess ranking quality, and
    set $M = 100$. Therefore, Hits@K = 0.1 means that on average, the model ranks a positive
    edge within the highest-ranked $K$ personalized negatives $10\%$ of the time.
    \item \textbf{MRR} (Mean Reciprocal Rank): Computes the average reciprocal rank of positive
    edges among their personalized negative samples~\cite{Li2023} (the same as those used for Hits@K)
    providing an overall measure of ranking accuracy. For instance, $MRR = 0.1$ means that on average,
    the model ranks a positive edge as $10^{\text{th}}$ among $M$ personalized negative samples.
\end{itemize}

Each metric ranges from 0 to 1, with higher values reflecting improved performance.
Based on prior work, we expect AUC and AUPRC scores to approach 1, while MRR and Hits@K often
yield values around 0.15 for similar undirected tasks~\cite{Li2023}. \emph{Directional} and
\emph{Bidirectional} performances were evaluated using only AUC and AUPRC, since Hits@K and MRR
are hardly definable in those scenarios. The number of training epochs was optimized through
Early Stopping on the \emph{General} validation set performance (given by the sum of AUC and AUPRC).

\subsubsection{Results}

The results below represent average performance over six random splits of $E_{\text{train}}$,
$E_{\text{val}}$, and $E_{\text{test}}$ while keeping the model's seed fixed for fair comparison.
The \emph{training / validation / test} split proportions are:

\begin{itemize}
    \item $85 / 5 / 10$ for unidirectional edges,
    \item $65 / 15 / 30$ for bidirectional edges.
\end{itemize}

\begin{table}[h]
\centering
\makebox[\linewidth][c]{
\resizebox{1.15\textwidth}{!}{
\begin{tabular}{lcccccccc}
\hline
\textbf{Model} & \textbf{G-ROC-AUC} & \textbf{G-AUPRC} & \textbf{G-Hits@10} & \textbf{G-MRR} & \textbf{D-ROC-AUC} & \textbf{D-AUPRC} & \textbf{B-ROC-AUC} & \textbf{B-AUPRC} \\
\hline
\texttt{gae}              & $0.8484 \pm 9 \times 10^{-4}$  & $0.8558 \pm 6 \times 10^{-4}$   & $0.0001 \pm 4 \times 10^{-5}$ & $0.0165 \pm 2 \times 10^{-4}$ & $0.5 \pm 0$          & $0.5 \pm 0$          & $0.941 \pm 5 \times 10^{-3}$    & $0.964 \pm 3 \times 10^{-3}$    \\
\texttt{gr\_gae}      & $0.9806 \pm 3 \times 10^{-4}$  & $0.9753 \pm 4 \times 10^{-4}$   & $0.17 \pm 3 \times 10^{-3}$   & $0.112 \pm 5 \times 10^{-3}$  & $0.9958 \pm 1 \times 10^{-4}$   & $0.9874 \pm 2 \times 10^{-4}$   & $0.99717 \pm 4 \times 10^{-5}$  & $0.99431 \pm 7 \times 10^{-5}$  \\
\texttt{st\_gae} & $0.99976 \pm 1 \times 10^{-5}$ & $0.999736 \pm 8 \times 10^{-6}$ & $0.279 \pm 9 \times 10^{-3}$  & $0.152 \pm 5 \times 10^{-3}$  & $0.999982 \pm 1 \times 10^{-6}$ & $0.999983 \pm 1 \times 10^{-6}$ & $0.999989 \pm 2 \times 10^{-6}$ & $0.999987 \pm 3 \times 10^{-6}$ \\
\texttt{mlp\_gae}          & $0.99315 \pm 1 \times 10^{-5}$ & $0.99409 \pm 1 \times 10^{-5}$  & $0.53 \pm 2 \times 10^{-3}$   & $0.289 \pm 6 \times 10^{-3}$  & $0.99671 \pm 2 \times 10^{-5}$  & $0.9973 \pm 1 \times 10^{-5}$   & $0.99692 \pm 2 \times 10^{-5}$  & $0.99736 \pm 2 \times 10^{-5}$  \\
\texttt{digae}            & $0.9978 \pm 3 \times 10^{-4}$  & $0.998 \pm 3 \times 10^{-4}$    & $0.18 \pm 2 \times 10^{-2}$   & $0.091 \pm 1 \times 10^{-2}$  & $0.9991 \pm 2 \times 10^{-4}$   & $0.9993 \pm 2 \times 10^{-4}$   & $0.9994 \pm 1 \times 10^{-4}$   & $0.9995 \pm 1 \times 10^{-4}$   \\
\texttt{magnet}           & $0.989 \pm 1 \times 10^{-4}$   & $0.99076 \pm 3 \times 10^{-5}$  & $0.38 \pm 2 \times 10^{-2}$   & $0.24 \pm 1 \times 10^{-2}$   & $0.9962 \pm 1 \times 10^{-3}$   & $0.9969 \pm 6 \times 10^{-4}$   & $0.9976 \pm 4 \times 10^{-4}$   & $0.9979 \pm 2 \times 10^{-4}$   \\
\hline
\end{tabular}
}}
\vspace{0.1em}
\caption{Results for various graph autoencoder models.}
\end{table}

\subsubsection{Discussion}

We observe consistently high \emph{General} AUC and AUPRC scores, close to 1.
These high values are expected, as similar neural architectures have demonstrated strong performance
in graphs of comparable size~\cite{Kipf2016}. The high ratio of existing to non-existing edges in the
implication graph (approximately 37\%) likely contributes to the near-perfect \emph{General} AUC and
AUPRC scores. For context, benchmark datasets such as Cora and Citeseer
(e.g., \href{https://github.com/deezer/gravity_graph_autoencoders/tree/master/data}{directed}
and \href{https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html}{undirected})
contain fewer than 1\% of all possible edges.

Interestingly, the GAE model, though structurally unable to distinguish edge direction, performs well
on the \emph{General} task (if we consider AUC and AUPRC only). This experimentally confirms the need
for including \emph{Directional} and \emph{Bidirectional} test sets, which allow comprehensive
evaluation across all facets of Directed Link Prediction (DLP).

All other models demonstrate high AUC and AUPRC scores across the \emph{General}, \emph{Directional},
and \emph{Bidirectional} test sets, indicating strong predictive capabilities even when accounting for
directionality and bidirectionality.

Notably, the \texttt{mlp\_gae} and \texttt{magnet} models also achieve competitive scores in MRR and
Hits@K metrics.

\subsubsection{Conclusions}

We evaluated the performance of six different graph autoencoders on a Directed Link Prediction (DLP) task.
By adopting a multi-task evaluation framework, we assessed the models comprehensively across various
aspects of DLP. All models demonstrated strong performance on AUC and AUPRC metrics, with some also
achieving high scores on MRR and Hits@K.

Node features were represented using one-hot encodings, meaning that the models received no explicit
information about the equations represented by the nodes. Instead, they relied entirely on the
topological structure encoded during training. This approach aligns with previous research suggesting
that one-hot encodings can promote asymmetric embeddings~\cite{Salha2019}. However, future experiments
could explore alternative representations, such as encoding the equations with text-based embeddings
like Word2Vec, to potentially enhance the models' understanding of the nodes' semantic content.

In summary, our findings highlight the adaptability and robustness of graph autoencoders for DLP
tasks in dense, directed graphs. This approach not only demonstrates robustness in predicting directed
links but also suggests promising potential for future improvements through enhanced feature
representations, thereby advancing the capabilities of link prediction in complex mathematical
knowledge graphs.
